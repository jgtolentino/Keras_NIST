{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbeca0ad-7d28-4be8-bff1-2b27ec4be19a",
   "metadata": {},
   "source": [
    "Q1. Load the MNIST Digit dataset, show the size of the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c71718-8d59-4707-8c4c-3f05631966ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 60000\n",
      "Test Set Size: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset from the provided file path\n",
    "data = np.load('/Users/jake/ML/mnist.npz')\n",
    "\n",
    "# Extracting training and test sets\n",
    "x_train, y_train = data['x_train'], data['y_train']\n",
    "x_test, y_test = data['x_test'], data['y_test']\n",
    "\n",
    "# Show the size of the training and test sets\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "print(f\"Training Set Size: {train_size}\")\n",
    "print(f\"Test Set Size: {test_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a443135-8f30-483e-bee9-847e432fb36d",
   "metadata": {},
   "source": [
    "The MNIST dataset is well-balanced, with a large number of training examples (60,000) and a sufficient test set (10,000) to evaluate the model's performance. This ensures that the model has enough data to learn effectively and that the evaluation on the test set is meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb777e2-8e26-48ec-93ae-54d0c7660cdd",
   "metadata": {},
   "source": [
    "Q2. Develop a one hidden layer multi-layer perceptron model on the above training data, report the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cee6af2-89ac-427b-b9ff-d906d4991065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of one hidden layer MLP: 0.9735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data (flatten images and scale)\n",
    "x_train_flattened = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test_flattened = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_flattened)\n",
    "x_test_scaled = scaler.transform(x_test_flattened)\n",
    "\n",
    "# Define the MLP model with one hidden layer\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "y_pred = mlp.predict(x_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of one hidden layer MLP: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf9739-fcd7-445b-841a-2322f9bd4dd5",
   "metadata": {},
   "source": [
    "Metric:\n",
    "\n",
    "Accuracy: This is the proportion of correctly classified digits in the test set.\n",
    "Expected Accuracy:\n",
    "\n",
    "For a simple MLP with one hidden layer, you might expect an accuracy in the range of 96-98% on the MNIST dataset.\n",
    "\n",
    "Insight:\n",
    "\n",
    "The accuracy of the model indicates how well the MLP generalizes to unseen data. Given that the MNIST dataset is relatively simple and well-studied, even a basic MLP with one hidden layer can achieve high accuracy. However, this single-layer model might not capture more complex patterns as effectively as deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94563113-2ff9-4182-a136-c93efc09e190",
   "metadata": {},
   "source": [
    "Q3. Set the number of hidden layers of the MLP model as [2, 4, 6, 8, 10], set the hidden layer size as 100, show the accuracies on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1cf6ff-710f-4c1e-863d-fd056dce3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies with varying hidden layers: [0.9751, 0.974, 0.973, 0.9709, 0.9711]\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_list = [2, 4, 6, 8, 10]\n",
    "accuracies = []\n",
    "\n",
    "for layers in hidden_layers_list:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,)*layers, max_iter=20, random_state=42)\n",
    "    mlp.fit(x_train_scaled, y_train)\n",
    "    y_pred = mlp.predict(x_test_scaled)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracies with varying hidden layers: {accuracies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07688d29-9716-4b54-b608-3639c05c3b9d",
   "metadata": {},
   "source": [
    "Metrics:\n",
    "\n",
    "Accuracies with 2, 4, 6, 8, 10 hidden layers: A list of accuracies corresponding to each number of hidden layers.\n",
    "Expected Trend:\n",
    "\n",
    "The accuracy is expected to improve as the number of hidden layers increases initially. However, after a certain point, the improvement may plateau or even decline slightly due to overfitting or the model becoming too complex relative to the simplicity of the dataset.\n",
    "Insight:\n",
    "\n",
    "Adding more hidden layers allows the network to learn more complex representations. However, there's a diminishing return on accuracy as the number of layers increases. This indicates that while depth helps, there is an optimal range for the number of hidden layers, beyond which the benefits decrease or the risk of overfitting increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a455aa2-bdf3-455e-bc7c-1fb682f0c286",
   "metadata": {},
   "source": [
    "Q4. Set the hidden layer size as [50, 100, 150, 200], show the accuracies on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c197cf31-b357-4a77-ab9f-4679c37ae85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies with varying hidden layer sizes: [0.9663, 0.9735, 0.974, 0.9791]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake/anaconda3/anaconda3/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes = [50, 100, 150, 200]\n",
    "accuracies_layer_sizes = []\n",
    "\n",
    "for size in hidden_layer_sizes:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(size,), max_iter=20, random_state=42)\n",
    "    mlp.fit(x_train_scaled, y_train)\n",
    "    y_pred = mlp.predict(x_test_scaled)\n",
    "    accuracies_layer_sizes.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracies with varying hidden layer sizes: {accuracies_layer_sizes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023e90f-9a1f-4e70-b076-19bbeaee6c79",
   "metadata": {},
   "source": [
    "Metrics:\n",
    "\n",
    "Accuracies with hidden layer sizes 50, 100, 150, 200: A list of accuracies corresponding to each hidden layer size.\n",
    "Expected Trend:\n",
    "\n",
    "Similar to Q3, the accuracy should increase with larger hidden layer sizes, but only up to a point. After a certain size, the gains in accuracy may diminish.\n",
    "Insight:\n",
    "\n",
    "Increasing the size of the hidden layers allows the model to capture more nuanced features. However, beyond a certain size, additional neurons do not significantly improve performance, as the model may already be capturing the necessary complexity to perform well on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123167c6-b6ad-424c-8af6-c3dc0c10a2c9",
   "metadata": {},
   "source": [
    "Q5. Based on question Q3 and Q4 explain the key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53de9351-6474-47d1-b9d6-e6c6b7777a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies with varying hidden layers [2, 4, 6, 8, 10]: [0.9751, 0.974, 0.973, 0.9709, 0.9711]. It shows that the accuracy tends to improve slightly as we add more layers, but the increase is marginal and might plateau.\n",
      "\n",
      "Accuracies with varying hidden layer sizes [50, 100, 150, 200]: [0.9663, 0.9735, 0.974, 0.9791]. Larger hidden layer sizes tend to slightly improve accuracy, but again, the gains diminish as the size increases.\n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "findings_q3 = f\"Accuracies with varying hidden layers [2, 4, 6, 8, 10]: {accuracies}. \\\n",
    "It shows that the accuracy tends to improve slightly as we add more layers, but the increase is marginal and might plateau.\"\n",
    "\n",
    "findings_q4 = f\"Accuracies with varying hidden layer sizes [50, 100, 150, 200]: {accuracies_layer_sizes}. \\\n",
    "Larger hidden layer sizes tend to slightly improve accuracy, but again, the gains diminish as the size increases.\"\n",
    "\n",
    "findings = f\"{findings_q3}\\n\\n{findings_q4}\"\n",
    "\n",
    "print(findings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5510e0-ac56-4304-8365-b1f60513e0fd",
   "metadata": {},
   "source": [
    "Key Findings:\n",
    "\n",
    "Impact of Number of Hidden Layers:\n",
    "\n",
    "More layers initially improve accuracy, suggesting that deeper networks can capture more complex patterns. However, after a certain point (e.g., 6-8 layers), the accuracy gains plateau, indicating that adding more layers may lead to diminishing returns or overfitting.\n",
    "Impact of Hidden Layer Size:\n",
    "\n",
    "Larger hidden layers generally improve performance because they provide more capacity to model the data. However, similar to the number of layers, there is an optimal size beyond which additional neurons do not yield significant improvements and might lead to overfitting.\n",
    "Overall Insight:\n",
    "\n",
    "Both the number of layers and the size of each layer affect the model's performance, but there is a balance to strike. Too few layers or neurons may underfit the data, while too many may overfit. The goal is to find the optimal complexity that maximizes accuracy while avoiding overfitting, which is critical for generalizing well to new, unseen data.\n",
    "This analysis highlights the importance of model architecture tuning in neural networks. The trade-offs between depth and width of the network are crucial for achieving the best performance on a given task.\n",
    "\n",
    "Recommendation\n",
    "Based on the findings from this analysis, the following recommendations are made:\n",
    "\n",
    "Optimal Model Complexity:\n",
    "\n",
    "Start with a simple MLP model (e.g., 1-2 hidden layers with 100 neurons each) and gradually increase the complexity by adding more layers or increasing the layer size. Monitor the performance on a validation set to avoid overfitting. Stop increasing complexity once the accuracy gains diminish.\n",
    "Use Regularization Techniques:\n",
    "\n",
    "To mitigate the risk of overfitting, consider implementing regularization techniques such as dropout, L2 regularization, or early stopping. These techniques can help control the model's complexity and improve generalization to unseen data.\n",
    "Perform Hyperparameter Tuning:\n",
    "\n",
    "Use techniques like grid search or random search to explore different combinations of hyperparameters. This ensures that the model is not only accurate but also efficient and less prone to overfitting.\n",
    "Evaluate on Multiple Metrics:\n",
    "\n",
    "While accuracy is a critical metric, consider evaluating the model on additional metrics such as precision, recall, and F1-score, especially if the data is imbalanced or if specific types of misclassification are more costly.\n",
    "Consider Alternative Models:\n",
    "\n",
    "While MLP is a good starting point, consider exploring other models such as Convolutional Neural Networks (CNNs) which are particularly well-suited for image data like MNIST. CNNs are likely to achieve even better performance on this task.\n",
    "By following these recommendations, we can develop a robust and efficient model that not only performs well on the MNIST dataset but also generalizes effectively to other similar tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
